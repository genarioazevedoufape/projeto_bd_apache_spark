---

# ğŸ” DeduplicaÃ§Ã£o DistribuÃ­da de Registros com Apache Spark

Este repositÃ³rio apresenta uma abordagem distribuÃ­da para **deduplicaÃ§Ã£o de registros em larga escala**, implementada com **Apache Spark**.
O projeto foi desenvolvido como parte de uma pesquisa acadÃªmica sobre tÃ©cnicas de *Entity Resolution (ER)* aplicadas em cenÃ¡rios de **Big Data**.

---

## ğŸ“Œ MotivaÃ§Ã£o

A duplicidade de registros compromete a qualidade de dados em bases massivas, impactando diretamente anÃ¡lises, tomada de decisÃ£o e eficiÃªncia operacional.
Tradicionalmente, mÃ©todos de deduplicaÃ§Ã£o exigem comparaÃ§Ãµes exaustivas (*O(nÂ²)*), inviÃ¡veis para milhÃµes de registros.

Neste trabalho, avaliamos estratÃ©gias de **blocking** e **similaridade** para equilibrar **qualidade** (F1-Score) e **eficiÃªncia computacional** (tempo de execuÃ§Ã£o e throughput).

---

## âš™ï¸ Pipeline Proposto

![Pipeline](Diagrama.png)

1. **GeraÃ§Ã£o de Dados SintÃ©ticos**
2. **PrÃ©-processamento** (normalizaÃ§Ã£o de texto)
3. **Blocking Strategy**

   * Standard Blocking
   * Sorted Neighborhood
   * Hash-based Windowed
4. **GeraÃ§Ã£o de Pares Candidatos**
5. **CÃ¡lculo de Similaridade**

   * WRatio (baseado em Levenshtein)
   * Jaccard (token-based)
6. **AvaliaÃ§Ã£o**

   * PrecisÃ£o
   * Recall
   * F1-Score
7. **Resultados e AnÃ¡lise**

---

## ğŸ› ï¸ Tecnologias Utilizadas

* **Apache Spark 3.5.1**
* **Python 3.10+**
* **PySpark** (processamento distribuÃ­do)
* **RapidFuzz** (similaridade de strings)
* **Pandas / Matplotlib / Seaborn** (anÃ¡lise e visualizaÃ§Ã£o)
* **Faker** (geraÃ§Ã£o de dados sintÃ©ticos)
* Ambiente: **Google Colab**

---

## ğŸ“Š Conjuntos de Dados

Foram gerados datasets sintÃ©ticos com **20% de duplicatas**, em trÃªs escalas:

* **10 mil registros**
* **100 mil registros**
* **1 milhÃ£o de registros**

---

## ğŸš€ ExecuÃ§Ã£o

### PrÃ©-requisitos

* Python 3.10+
* Apache Spark 3.5.1
* Instalar dependÃªncias:

```bash
pip install pyspark pandas rapidfuzz matplotlib seaborn faker
```

### Rodando no Google Colab

1. Clone este repositÃ³rio:

```bash
git clone https://github.com/SEU_USUARIO/deduplicacao-spark.git
```

2. Abra o notebook:

```bash
10k,100k,1m.ipynb
```

3. Execute as cÃ©lulas para reproduzir os experimentos.

---

## ğŸ“ˆ Resultados Principais

* **Standard Blocking + WRatio** obteve o **maior F1-score (0,725)** em 10k registros.
* Entretanto, essa estratÃ©gia apresentou queda brusca em bases maiores (**F1 = 0,035 em 1M**).
* **Hash-based Windowed Blocking** foi a **mais escalÃ¡vel**, processando 1M registros com melhor eficiÃªncia temporal e uso de memÃ³ria.
* Foi observada a compensaÃ§Ã£o (*trade-off*) entre **qualidade** e **eficiÃªncia**:

  * **Standard Blocking** â†’ Alta qualidade, baixa escalabilidade.
  * **Hash-based Windowed** â†’ Maior eficiÃªncia, menor qualidade.

---

## ğŸ“š ReferÃªncias

* Christen, P. *Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection*. Springer, 2012.
* Zaharia, M. et al. *Apache Spark: The Definitive Guide*. Oâ€™Reilly Media, 2018.
* Kolb, L.; Thor, A.; Rahm, E. *Dedoop: Efficient Deduplication with Hadoop*. VLDB Endowment, 2012.

---

## âœ¨ Autores

* GenÃ¡rio C. Azevedo
* Matheus Henrique A. Gomes
* RomÃ¡rio A. FranÃ§a

---
